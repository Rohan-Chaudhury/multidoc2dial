{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/r/rohan.chaudhury/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-28 20:13:07.516443: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 20:13:07.634794: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-28 20:13:08.115650: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 20:13:08.115733: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 20:13:08.115739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    DPRContextEncoder, DPRContextEncoderTokenizerFast,\n",
    "    DPRQuestionEncoder, DPRQuestionEncoderTokenizerFast,\n",
    "    DPRReader, DPRReaderTokenizerFast, TrainingArguments, Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/data/mdd_dpr/dpr.multidoc2dial_all.structure.train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/data/mdd_dpr/dpr.psg.multidoc2dial_all.structure.json\", \"r\") as f:\n",
    "    corpus_data = json.load(f)\n",
    "\n",
    "with open(\"/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/data/mdd_dpr/dpr.multidoc2dial_all.structure.validation.json\", \"r\") as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "with open(\"/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/data/mdd_dpr/dpr.multidoc2dial_all.structure.test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_data(train_data, corpus_data):\n",
    "#     contexts = []\n",
    "#     questions = []\n",
    "#     answers = []\n",
    "\n",
    "#     id2text = {item[\"id\"]: item[\"text\"] for item in corpus_data}\n",
    "\n",
    "#     for data in train_data:\n",
    "#         question = data[\"question\"]\n",
    "#         answer = data[\"answers\"][0]\n",
    "#         positive_ctx = data[\"positive_ctxs\"][0][\"psg_id\"]\n",
    "\n",
    "#         questions.append(question)\n",
    "#         answers.append(answer)\n",
    "#         contexts.append(id2text[positive_ctx])\n",
    "\n",
    "#     return contexts, questions, answers\n",
    "\n",
    "# contexts, questions, answers = preprocess_data(train_data, corpus_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 5.46kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.05MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.11MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 492/492 [00:00<00:00, 431kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:06<00:00, 64.8MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 15.0kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.96MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.00MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 493/493 [00:00<00:00, 296kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:06<00:00, 69.8MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 12.6kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.05MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.06MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 484/484 [00:00<00:00, 468kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRReaderTokenizerFast'.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:01<00:00, 253MB/s]  \n",
      "Some weights of the model checkpoint at facebook/dpr-reader-single-nq-base were not used when initializing DPRReader: ['span_predictor.encoder.bert_model.pooler.dense.weight', 'span_predictor.encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRReader from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRReader from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ctx_encoder_model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(ctx_encoder_model_name)\n",
    "ctx_model = DPRContextEncoder.from_pretrained(ctx_encoder_model_name)\n",
    "\n",
    "question_encoder_model_name = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "question_tokenizer = DPRQuestionEncoderTokenizerFast.from_pretrained(question_encoder_model_name)\n",
    "question_model = DPRQuestionEncoder.from_pretrained(question_encoder_model_name)\n",
    "\n",
    "reader_model_name = \"facebook/dpr-reader-single-nq-base\"\n",
    "reader_tokenizer = DPRReaderTokenizerFast.from_pretrained(reader_model_name)\n",
    "reader_model = DPRReader.from_pretrained(reader_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "class DPRTripletDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item[\"question\"]\n",
    "        positive_ctx = item[\"positive_ctxs\"][0][\"text\"]\n",
    "        negative_ctx = random.choice(item[\"negative_ctxs\"])[\"text\"]\n",
    "        hard_negative_ctx = random.choice(item[\"hard_negative_ctxs\"])[\"text\"]\n",
    "\n",
    "        question_input = self.tokenizer(question, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        positive_ctx_input = self.tokenizer(positive_ctx, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        negative_ctx_input = self.tokenizer(negative_ctx, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        hard_negative_ctx_input = self.tokenizer(hard_negative_ctx, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"question_input\": question_input,\n",
    "            \"positive_ctx_input\": positive_ctx_input,\n",
    "            \"negative_ctx_input\": negative_ctx_input,\n",
    "            \"hard_negative_ctx_input\": hard_negative_ctx_input,\n",
    "        }\n",
    "\n",
    "batch_size = 8\n",
    "max_length = 512\n",
    "# Initialize the dataset and DataLoader\n",
    "train_dataset = DPRTripletDataset(train_data, tokenizer=question_tokenizer,  max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = DPRTripletDataset(validation_data, tokenizer=question_tokenizer,  max_length=max_length)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = DPRTripletDataset(test_data, tokenizer=question_tokenizer,  max_length=max_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"  # Set this to the index of the GPU you want to use\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "question_model = nn.DataParallel(question_model)\n",
    "ctx_model = nn.DataParallel(ctx_model)\n",
    "question_model.to(device)\n",
    "ctx_model.to(device)\n",
    "\n",
    "num_epochs = 4\n",
    "lr = 1e-5\n",
    "optimizer_q = AdamW(question_model.parameters(), lr=lr)\n",
    "optimizer_ctx = AdamW(ctx_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2682/2682 [45:08<00:00,  1.01s/it]\n",
      "Validation: 100%|██████████| 526/526 [03:06<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Average training loss: nan\n",
      "Validation loss: nan\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2682/2682 [45:06<00:00,  1.01s/it]\n",
      "Validation: 100%|██████████| 526/526 [03:06<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Average training loss: nan\n",
      "Validation loss: nan\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2682/2682 [45:02<00:00,  1.01s/it]\n",
      "Validation: 100%|██████████| 526/526 [03:07<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Average training loss: nan\n",
      "Validation loss: nan\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 1655/2682 [27:52<17:17,  1.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb Cell 8\u001b[0m in \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Gradient accumulation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m gradient_accumulation_steps\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m absolute_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m gradient_accumulation_steps\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m gradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "# Gradient clipping value\n",
    "grad_clip = 1.0\n",
    "# Epsilon value to avoid division by zero\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Define gradient accumulation steps\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "def calculate_validation_loss(question_model, ctx_model, val_dataloader):\n",
    "    question_model.eval()\n",
    "    ctx_model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "            question_input = {k: v.squeeze(1).to(device) for k, v in batch[\"question_input\"].items()}\n",
    "            positive_ctx_input = {k: v.squeeze(1).to(device) for k, v in batch[\"positive_ctx_input\"].items()}\n",
    "            negative_ctx_input = {k: v.squeeze(1).to(device) for k, v in batch[\"negative_ctx_input\"].items()}\n",
    "            hard_negative_ctx_input = {k: v.squeeze(1).to(device) for k, v in batch[\"hard_negative_ctx_input\"].items()}\n",
    "            \n",
    "            q_embeddings = question_model(**question_input).pooler_output\n",
    "            pos_ctx_embeddings = ctx_model(**positive_ctx_input).pooler_output\n",
    "            neg_ctx_embeddings = ctx_model(**negative_ctx_input).pooler_output\n",
    "            hard_neg_ctx_embeddings = ctx_model(**hard_negative_ctx_input).pooler_output\n",
    "\n",
    "            pos_scores = torch.matmul(q_embeddings, pos_ctx_embeddings.T).diagonal()\n",
    "            neg_scores = torch.matmul(q_embeddings, neg_ctx_embeddings.T).diagonal()\n",
    "            hard_neg_scores = torch.matmul(q_embeddings, hard_neg_ctx_embeddings.T).diagonal()\n",
    "\n",
    "            # Modify the loss calculation by adding epsilon\n",
    "            loss = -torch.log(pos_scores / (pos_scores + neg_scores + hard_neg_scores + epsilon))\n",
    "            total_loss += loss.mean().item()\n",
    "\n",
    "\n",
    "    return total_loss / len(val_dataloader)\n",
    "\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    question_model.train()\n",
    "    ctx_model.train()\n",
    "\n",
    "    optimizer_q.zero_grad()\n",
    "    optimizer_ctx.zero_grad()\n",
    "    absolute_loss = 0.0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        question_input = {k: v.squeeze(1).to(device) for k, v in batch[\"question_input\"].items()}\n",
    "        positive_ctx_input = {k: v.squeeze(1).to(device) for k, v in batch[\"positive_ctx_input\"].items()}\n",
    "        negative_ctx_input = {k: v.squeeze(1).to(device) for k, v in batch[\"negative_ctx_input\"].items()}\n",
    "        hard_negative_ctx_input = {k: v.squeeze(1).to(device) for k, v in batch[\"hard_negative_ctx_input\"].items()}\n",
    "        \n",
    "        q_embeddings = question_model(**question_input).pooler_output\n",
    "        pos_ctx_embeddings = ctx_model(**positive_ctx_input).pooler_output\n",
    "        neg_ctx_embeddings = ctx_model(**negative_ctx_input).pooler_output\n",
    "        hard_neg_ctx_embeddings = ctx_model(**hard_negative_ctx_input).pooler_output\n",
    "\n",
    "        pos_scores = torch.matmul(q_embeddings, pos_ctx_embeddings.T).diagonal()\n",
    "        neg_scores = torch.matmul(q_embeddings, neg_ctx_embeddings.T).diagonal()\n",
    "        hard_neg_scores = torch.matmul(q_embeddings, hard_neg_ctx_embeddings.T).diagonal()\n",
    "\n",
    "        loss = -torch.log(pos_scores / (pos_scores + neg_scores + hard_neg_scores + epsilon))\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        loss /= gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        absolute_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            nn_utils.clip_grad_norm_(question_model.parameters(), grad_clip)\n",
    "            nn_utils.clip_grad_norm_(ctx_model.parameters(), grad_clip)\n",
    "            optimizer_q.step()\n",
    "            optimizer_ctx.step()\n",
    "            optimizer_q.zero_grad()\n",
    "            optimizer_ctx.zero_grad()\n",
    "\n",
    "    # Validation loop\n",
    "    average_loss = absolute_loss / len(train_dataloader)\n",
    "    val_loss = calculate_validation_loss(question_model, ctx_model, val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average training loss: {average_loss}\")\n",
    "\n",
    "    print(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(question_model.state_dict(), os.path.join(output_dir, \"best_question_model.pth\"))\n",
    "        torch.save(ctx_model.state_dict(), os.path.join(output_dir, \"best_ctx_model.pth\"))\n",
    "        print(f\"Best model saved at epoch {epoch + 1}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the Dense Retriever (DPR), we generally use the positive context and a subset of negative contexts for each question to calculate the loss. The subset of negative contexts usually includes random negative contexts and hard negative contexts. Using the entire corpus for each item would make the training process extremely slow and computationally expensive.\n",
    "\n",
    "The idea is to sample a few negative contexts for each question and learn to distinguish between the positive context and these negative contexts. The random negative contexts are usually irrelevant to the question, while hard negative contexts are slightly relevant but do not correctly answer the question.\n",
    "\n",
    "By training the model using this approach, the model learns to rank the positive context higher than the negative contexts, even though it does not see the entire corpus during training. Once the model is trained, you can use it to retrieve the most relevant context from the entire corpus during the inference phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_index(big_dataset, context_text):\n",
    "    for idx, item in enumerate(big_dataset):\n",
    "        if item[\"text\"] == context_text:\n",
    "            return idx\n",
    "    print(\"Context not found!\")\n",
    "    return -1\n",
    "\n",
    "# Add correct_context_idx to test_data\n",
    "for item in test_data:\n",
    "    positive_context = item[\"positive_ctxs\"][0][\"text\"]\n",
    "    correct_context_idx = find_context_index(corpus_data, positive_context)\n",
    "    item[\"correct_context_idx\"] = correct_context_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/best_question_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb Cell 12\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m question_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(output_dir, \u001b[39m\"\u001b[39;49m\u001b[39mbest_question_model.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m ctx_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, \u001b[39m\"\u001b[39m\u001b[39mbest_ctx_model.pth\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m question_model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/best_question_model.pth'"
     ]
    }
   ],
   "source": [
    "question_model.load_state_dict(torch.load(os.path.join(output_dir, \"best_question_model.pth\")))\n",
    "ctx_model.load_state_dict(torch.load(os.path.join(output_dir, \"best_ctx_model.pth\")))\n",
    "\n",
    "question_model.eval()\n",
    "ctx_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb Cell 13\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m all_context_inputs \u001b[39m=\u001b[39m question_tokenizer(all_contexts, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m all_context_inputs \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m all_context_inputs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m all_context_embeddings \u001b[39m=\u001b[39m ctx_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_context_inputs)\u001b[39m.\u001b[39mpooler_output\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Test loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/t5_rag_dpr.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m top1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/transformers/models/dpr/modeling_dpr.py:506\u001b[0m, in \u001b[0;36mDPRContextEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m--> 506\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx_encoder(\n\u001b[1;32m    507\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    508\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    509\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    510\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    511\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    512\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    513\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    514\u001b[0m )\n\u001b[1;32m    516\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    517\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/transformers/models/dpr/modeling_dpr.py:196\u001b[0m, in \u001b[0;36mDPREncoder.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    187\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    188\u001b[0m     input_ids: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     return_dict: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    195\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[BaseModelOutputWithPooling, Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]:\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_model(\n\u001b[1;32m    197\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    198\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    199\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    200\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    201\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    202\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    203\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m     sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    206\u001b[0m     pooled_output \u001b[39m=\u001b[39m sequence_output[:, \u001b[39m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1016\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1017\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[1;32m   1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1021\u001b[0m     embedding_output,\n\u001b[1;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:230\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    227\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    231\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    233\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "class DPRQuestionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item[\"question\"]\n",
    "\n",
    "        question_input = self.tokenizer(question, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return question_input\n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "test_question_dataset = DPRQuestionDataset(test_data, tokenizer=question_tokenizer)\n",
    "test_question_dataloader = DataLoader(test_question_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Embed all contexts from the entire dataset\n",
    "all_contexts = [item[\"text\"] for item in corpus_data]\n",
    "all_context_inputs = question_tokenizer(all_contexts, max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "all_context_inputs = {k: v.to(device) for k, v in all_context_inputs.items()}\n",
    "all_context_embeddings = ctx_model(**all_context_inputs).pooler_output\n",
    "\n",
    "# Test loop\n",
    "top1 = 0\n",
    "top5 = 0\n",
    "top10 = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_question_dataloader, desc=\"Testing\"):\n",
    "        question_input = {k: v.to(device) for k, v in batch.items()}\n",
    "        q_embeddings = question_model(**question_input).pooler_output\n",
    "        scores = torch.matmul(q_embeddings, all_context_embeddings.T)\n",
    "\n",
    "        top_k = scores.argsort(descending=True)\n",
    "        # Assuming you have the correct context index in the test data\n",
    "        correct_context_indices = [item[\"correct_context_idx\"] for item in test_data]\n",
    "\n",
    "        for i, top_idx in enumerate(top_k):\n",
    "            if correct_context_indices[i] in top_idx[:1]:\n",
    "                top1 += 1\n",
    "            if correct_context_indices[i] in top_idx[:5]:\n",
    "                top5 += 1\n",
    "            if correct_context_indices[i] in top_idx[:10]:\n",
    "                top10 += 1\n",
    "\n",
    "total = len(test_data)\n",
    "top1_accuracy = top1 / total\n",
    "top5_accuracy = top5 / total\n",
    "top10_accuracy = top10 / total\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy * 100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy * 100:.2f}%\")\n",
    "print(f\"Top-10 Accuracy: {top10_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlpproj': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6d18fc597d68f35882868caff42bd3974688c44a4bcb8df45c7c533b1bd427d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
