{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/r/rohan.chaudhury/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-29 03:21:04.174684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 03:21:04.290495: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-29 03:21:04.752224: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-29 03:21:04.752310: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-29 03:21:04.752316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    DPRContextEncoder, DPRContextEncoderTokenizerFast,\n",
    "    DPRQuestionEncoder, DPRQuestionEncoderTokenizerFast,\n",
    "    DPRReader, DPRReaderTokenizerFast, TrainingArguments, Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRConfig, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRReader, DPRReaderTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "config = DPRConfig.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "reader = DPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\n",
    "reader_tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): DPRContextEncoder(\n",
       "    (ctx_encoder): DPREncoder(\n",
       "      (bert_model): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # Set this to the index of the GPU you want to use\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "question_encoder = nn.DataParallel(question_encoder)\n",
    "context_encoder = nn.DataParallel(context_encoder)\n",
    "question_encoder.to(device)\n",
    "context_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/data/mdd_dpr/dpr.multidoc2dial_all.structure.train.json\", \"r\") as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "with open(\"/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/data/mdd_dpr/dpr.psg.multidoc2dial_all.structure.json\", \"r\") as f:\n",
    "    corpus_data = json.load(f)\n",
    "\n",
    "with open(\"/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/data/mdd_dpr/dpr.multidoc2dial_all.structure.validation.json\", \"r\") as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "with open(\"/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/data/mdd_dpr/dpr.multidoc2dial_all.structure.test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def preprocess_question(question):\n",
    "    turns = question.split(\"[SEP]\")\n",
    "    questions=turns[0]\n",
    "    turns=[turns[1]]\n",
    "    turns = [turn.strip() for turn in turns if turn.strip()]\n",
    "    turns = [turn.split(\"||\") for turn in turns]\n",
    "    turns = [turn[::-1] for turn in turns]  # Reverse the order of previous turns\n",
    "    turns = [\" || \".join(turn) for turn in turns]\n",
    "\n",
    "    return \" \".join(turns).lower() + \" || \"+ \"user: \"+ questions.lower()\n",
    "\n",
    "\n",
    "\n",
    "# # Preprocess the training dataset\n",
    "# def preprocess_data(training_data):\n",
    "#     train_data = {\n",
    "#         \"question\": [],\n",
    "#         \"answers\": [],\n",
    "#         \"ctx_title\": [],\n",
    "#         \"ctx_text\": [],\n",
    "#         \"label\": []\n",
    "#     }\n",
    "    \n",
    "#     for item in training_data:\n",
    "#         question = preprocess_question(item[\"question\"])\n",
    "#         answers = [remove_extra_spaces(answer.lower()) for answer in item[\"answers\"]]\n",
    "#         positive_ctxs = item[\"positive_ctxs\"]\n",
    "#         negative_ctxs = item[\"negative_ctxs\"]\n",
    "#         hard_negative_ctxs = item[\"hard_negative_ctxs\"]\n",
    "        \n",
    "#         for positive_ctx in positive_ctxs:\n",
    "#             title = remove_extra_spaces(positive_ctx[\"title\"].lower())\n",
    "#             text = remove_extra_spaces(positive_ctx[\"text\"].lower())\n",
    "#             train_data[\"question\"].append(question)\n",
    "#             train_data[\"answers\"].append(answers)\n",
    "#             train_data[\"ctx_title\"].append(title)\n",
    "#             train_data[\"ctx_text\"].append(text)\n",
    "#             train_data[\"label\"].append(1)\n",
    "        \n",
    "#         for negative_ctx in negative_ctxs:\n",
    "#             title = remove_extra_spaces(negative_ctx[\"title\"].lower())\n",
    "#             text = remove_extra_spaces(negative_ctx[\"text\"].lower())\n",
    "#             train_data[\"question\"].append(question)\n",
    "#             train_data[\"answers\"].append(answers)\n",
    "#             train_data[\"ctx_title\"].append(title)\n",
    "#             train_data[\"ctx_text\"].append(text)\n",
    "#             train_data[\"label\"].append(0)\n",
    "        \n",
    "#         for hard_negative_ctx in hard_negative_ctxs:\n",
    "#             title = remove_extra_spaces(hard_negative_ctx[\"title\"].lower())\n",
    "#             text = remove_extra_spaces(hard_negative_ctx[\"text\"].lower())\n",
    "#             train_data[\"question\"].append(question)\n",
    "#             train_data[\"answers\"].append(answers)\n",
    "#             train_data[\"ctx_title\"].append(title)\n",
    "#             train_data[\"ctx_text\"].append(text)\n",
    "#             train_data[\"label\"].append(-1)\n",
    "    \n",
    "#     return train_data\n",
    "\n",
    "# def preprocess_corpus_data(corpus_data):\n",
    "#     corpus_data_preprocessed = {\n",
    "#         \"title\": [],\n",
    "#         \"text\": []\n",
    "#     }\n",
    "\n",
    "#     for item in corpus_data:\n",
    "#         title = remove_extra_spaces(item[\"title\"].lower())\n",
    "#         text = remove_extra_spaces(item[\"text\"].lower())\n",
    "#         corpus_data_preprocessed[\"title\"].append(title)\n",
    "#         corpus_data_preprocessed[\"text\"].append(text)\n",
    "    \n",
    "#     return corpus_data_preprocessed\n",
    "\n",
    "# corpus_data_dict = preprocess_corpus_data(corpus_data)\n",
    "# corpus_dataset = Dataset.from_dict(corpus_data_dict)\n",
    "\n",
    "\n",
    "# training_data= preprocess_data(training_data)\n",
    "# test_data= preprocess_data(test_data)\n",
    "# validation_data= preprocess_data(validation_data)\n",
    "\n",
    "# train_dataset = Dataset.from_dict(training_data)\n",
    "# test_dataset = Dataset.from_dict(test_data)\n",
    "# validation_dataset = Dataset.from_dict(validation_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(training_data):\n",
    "    train_data = {\n",
    "        \"question\": [],\n",
    "        \"context\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for item in training_data:\n",
    "        question = preprocess_question(item[\"question\"])\n",
    "        positive_ctxs = item[\"positive_ctxs\"]\n",
    "        negative_ctxs = item[\"negative_ctxs\"]\n",
    "        hard_negative_ctxs = item[\"hard_negative_ctxs\"]\n",
    "        \n",
    "        for positive_ctx in positive_ctxs:\n",
    "            context = remove_extra_spaces(positive_ctx[\"title\"].lower() + \" \" + positive_ctx[\"text\"].lower())\n",
    "            train_data[\"question\"].append(question)\n",
    "            train_data[\"context\"].append(context)\n",
    "            train_data[\"labels\"].append(2)\n",
    "        \n",
    "        for negative_ctx in negative_ctxs:\n",
    "            context = remove_extra_spaces(negative_ctx[\"title\"].lower() + \" \" + negative_ctx[\"text\"].lower())\n",
    "            train_data[\"question\"].append(question)\n",
    "            train_data[\"context\"].append(context)\n",
    "            train_data[\"labels\"].append(1)\n",
    "        \n",
    "        for hard_negative_ctx in hard_negative_ctxs:\n",
    "            context = remove_extra_spaces(hard_negative_ctx[\"title\"].lower() + \" \" + hard_negative_ctx[\"text\"].lower())\n",
    "            train_data[\"question\"].append(question)\n",
    "            train_data[\"context\"].append(context)\n",
    "            train_data[\"labels\"].append(0)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "def preprocess_corpus_data(corpus_data):\n",
    "    corpus_data_preprocessed = {\n",
    "        \"title\": [],\n",
    "        \"text\": []\n",
    "    }\n",
    "\n",
    "    for item in corpus_data:\n",
    "        title = remove_extra_spaces(item[\"title\"].lower())\n",
    "        text = remove_extra_spaces(item[\"text\"].lower())\n",
    "        corpus_data_preprocessed[\"title\"].append(title)\n",
    "        corpus_data_preprocessed[\"text\"].append(text)\n",
    "    \n",
    "    return corpus_data_preprocessed\n",
    "\n",
    "corpus_data_dict = preprocess_corpus_data(corpus_data)\n",
    "corpus_dataset = Dataset.from_dict(corpus_data_dict)\n",
    "\n",
    "# training_data= preprocess_data(training_data)\n",
    "# test_data= preprocess_data(test_data)\n",
    "# validation_data= preprocess_data(validation_data)\n",
    "\n",
    "training_data= preprocess_data(training_data)\n",
    "test_data= preprocess_data(test_data)\n",
    "validation_data= preprocess_data(validation_data)\n",
    "\n",
    "train_dataset = Dataset.from_dict(training_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "validation_dataset = Dataset.from_dict(validation_data)\n",
    "\n",
    "# Add the preprocessing function that tokenizes the context and question\n",
    "def preprocess_function(examples):\n",
    "    question_encodings = question_tokenizer(examples['question'], truncation=True, padding='max_length', max_length=256)\n",
    "    context_encodings = context_tokenizer(examples['context'], truncation=True, padding='max_length', max_length=256)\n",
    "    \n",
    "    encodings = {\n",
    "        'input_ids': question_encodings['input_ids'],\n",
    "        'attention_mask': question_encodings['attention_mask'],\n",
    "        'context_input_ids': context_encodings['input_ids'],\n",
    "        'context_attention_mask': context_encodings['attention_mask'],\n",
    "        'labels': examples['labels'],\n",
    "    }\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "# validation_dataset = validation_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "# class CustomDataset(TorchDataset):\n",
    "#     def __init__(self, data_dict):\n",
    "#         self.data_dict = data_dict\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: value[idx] for key, value in self.data_dict.items()}\n",
    "#         return preprocess_function(item)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data_dict[\"attention_mask\"])\n",
    "\n",
    "# train_dataset = CustomDataset(training_data)\n",
    "# validation_dataset = CustomDataset(validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': [' || user: hello, i forgot o update my address, can you help me with that?', ' || user: hello, i forgot o update my address, can you help me with that?'], 'context': ['top 5 dmv mistakes and how to avoid them#3_0 top 5 dmv mistakes and how to avoid them // 1. forgetting to update address by statute , you must report a change of address to dmv within ten days of moving. that is the case for the address associated with your license, as well as all the addresses associated with each registered vehicle, which may differ. it is not sufficient to only: write your new address on the back of your old license; tell the united states postal service; or inform the police officer writing you a ticket. if you fail to keep your address current , you will miss a suspension order and may be charged with operating an unregistered vehicle and/or aggravated unlicensed operation, both misdemeanors. this really happens , but the good news is this is a problem that is easily avoidable. learn more about how to change the address on your license and registrations [1 ]', 'fafsa® help | federal student aid#1_0 fafsa® help | federal student aid // trending topics which school year should i select if i am applying for a summer session? what do i do if i forgot my fsa id username and password? what are the deadlines for filling out the fafsa form? how do i correct my fafsa form? what income and tax information do i report on the 2020 21 fafsa form?'], 'labels': [1, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450471\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset[:][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.data.data_collator import DataCollator\n",
    "\n",
    "\n",
    "# class DPRDataCollator(DataCollator):\n",
    "#     def __init__(self, question_tokenizer, context_tokenizer):\n",
    "#         self.question_tokenizer = question_tokenizer\n",
    "#         self.context_tokenizer = context_tokenizer\n",
    "\n",
    "#     def __call__(self, batch):\n",
    "#         question_texts = [item[\"question\"] for item in batch]\n",
    "#         context_texts = [item[\"context\"] for item in batch]\n",
    "#         labels = [item[\"label\"] for item in batch]\n",
    "\n",
    "#         question_inputs = self.question_tokenizer(question_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "#         context_inputs = self.context_tokenizer(context_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "#         batch_inputs = {\n",
    "#             \"input_ids\": question_inputs[\"input_ids\"],\n",
    "#             \"attention_mask\": question_inputs[\"attention_mask\"],\n",
    "#             \"context_input_ids\": context_inputs[\"input_ids\"],\n",
    "#             \"context_attention_mask\": context_inputs[\"attention_mask\"],\n",
    "#             \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "#         return batch_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " training code does not use the corpus_data. The corpus_data is typically used during the retrieval step when you use the trained DPR model to find relevant context passages to answer a given question. The training step focuses on learning to differentiate between relevant and irrelevant context passages based on the labeled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "\n",
    "# class DPRCombinedModel(nn.Module):\n",
    "#     def __init__(self, question_encoder, context_encoder):\n",
    "#         super(DPRCombinedModel, self).__init__()\n",
    "#         self.question_encoder = question_encoder\n",
    "#         self.context_encoder = context_encoder\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, context_input_ids, context_attention_mask):\n",
    "#         question_outputs = self.question_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         context_outputs = self.context_encoder(input_ids=context_input_ids, attention_mask=context_attention_mask)\n",
    "#         return question_outputs, context_outputs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder\n",
    "\n",
    "class DPRCombinedModel(nn.Module):\n",
    "    def __init__(self, question_encoder: DPRQuestionEncoder, context_encoder: DPRContextEncoder):\n",
    "        super(DPRCombinedModel, self).__init__()\n",
    "        self.question_encoder = question_encoder\n",
    "        self.context_encoder = context_encoder\n",
    "\n",
    "    def forward(self, question_input_ids, question_attention_mask, context_input_ids, context_attention_mask):\n",
    "        question_outputs = self.question_encoder(input_ids=question_input_ids, attention_mask=question_attention_mask)\n",
    "        context_outputs = self.context_encoder(input_ids=context_input_ids, attention_mask=context_attention_mask)\n",
    "        return question_outputs, context_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from transformers import Trainer\n",
    "\n",
    "# class DPRTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         print(inputs.keys())\n",
    "#         input_ids = inputs[\"input_ids\"]\n",
    "#         attention_mask = inputs[\"attention_mask\"]\n",
    "#         context_input_ids = inputs[\"context_input_ids\"]\n",
    "#         context_attention_mask = inputs[\"context_attention_mask\"]\n",
    "#         labels = inputs[\"labels\"]\n",
    "\n",
    "#         question_outputs, context_outputs = model(input_ids, attention_mask, context_input_ids, context_attention_mask)\n",
    "\n",
    "#         # Calculate dot product between question and context embeddings\n",
    "#         scores = torch.matmul(question_outputs.pooler_output, context_outputs.pooler_output.T)\n",
    "\n",
    "#         # Calculate the loss\n",
    "#         loss_fct = nn.CrossEntropyLoss()\n",
    "#         loss = loss_fct(scores, labels)\n",
    "\n",
    "#         return (loss, question_outputs, context_outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict, Tuple, Union\n",
    "# import torch\n",
    "\n",
    "# class DPRDataCollator:\n",
    "#     def __init__(self, question_tokenizer, context_tokenizer, max_length: int):\n",
    "#         self.question_tokenizer = question_tokenizer\n",
    "#         self.context_tokenizer = context_tokenizer\n",
    "#         self.max_length = max_length\n",
    "    \n",
    "#     def __call__(self, features):\n",
    "#         print(features[0].keys())\n",
    "#         input_ids = torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long)\n",
    "#         attention_mask = torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long)\n",
    "#         context_input_ids = torch.tensor([f[\"context_input_ids\"] for f in features], dtype=torch.long)\n",
    "#         context_attention_mask = torch.tensor([f[\"context_attention_mask\"] for f in features], dtype=torch.long)\n",
    "#         labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": input_ids,\n",
    "#             \"attention_mask\": attention_mask,\n",
    "#             \"context_input_ids\": context_input_ids,\n",
    "#             \"context_attention_mask\": context_attention_mask,\n",
    "#             \"labels\": labels,\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "# from transformers.data.data_collator import DataCollator\n",
    "\n",
    "\n",
    "# def train_dpr_model(train_dataset, gradient_accumulation_steps, per_device_train_batch_size, num_train_epochs, output_dir):\n",
    "#     combined_model = DPRCombinedModel(question_encoder, context_encoder)\n",
    "    \n",
    "#     training_args = TrainingArguments(\n",
    "#         gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         output_dir=output_dir,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"exact_match\",\n",
    "#         save_total_limit=1,\n",
    "#     )\n",
    "\n",
    "#     data_collator = DPRDataCollator(question_tokenizer, context_tokenizer, max_length=512)\n",
    "\n",
    "#     trainer = DPRTrainer(\n",
    "#         model=combined_model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         data_collator=data_collator,\n",
    "#         eval_dataset=validation_dataset,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "# print(train_dataset[:2].keys())\n",
    "# train_dpr_model(train_dataset, gradient_accumulation_steps=4, per_device_train_batch_size=4, num_train_epochs=3, output_dir=\"./dpr_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.optim import AdamW\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from math import ceil\n",
    "# def train_dpr_model(train_dataset, validation_dataset, model, question_tokenizer, context_tokenizer, epochs, batch_size, learning_rates, device, gradient_accumulation_steps=1, num_warmup_steps=0):\n",
    "#     # train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=DPRDataCollator(question_tokenizer, context_tokenizer, max_length=512))\n",
    "#     # val_dataloader = DataLoader(validation_dataset, batch_size=batch_size, collate_fn=DPRDataCollator(question_tokenizer, context_tokenizer, max_length=512))\n",
    "\n",
    "#     question_encoder_optimizer = AdamW(model.question_encoder.parameters(), lr=learning_rates['question_encoder'])\n",
    "#     context_encoder_optimizer = AdamW(model.context_encoder.parameters(), lr=learning_rates['context_encoder'])\n",
    "#     number_of_batches = ceil(len(train_dataset) / batch_size)\n",
    "#     total_steps = number_of_batches * epochs // gradient_accumulation_steps\n",
    "#     question_encoder_scheduler = get_linear_schedule_with_warmup(question_encoder_optimizer, num_warmup_steps, total_steps)\n",
    "#     context_encoder_scheduler = get_linear_schedule_with_warmup(context_encoder_optimizer, num_warmup_steps, total_steps)\n",
    "\n",
    "#     model.to(device)\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     max_length = 512\n",
    "#     for epoch in range(epochs):\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "#         model.train()\n",
    "#         total_loss = 0.0\n",
    "\n",
    "#         # for idx, batch in enumerate(train_dataloader):\n",
    "#         #     input_ids = batch[\"input_ids\"].to(device)\n",
    "#         #     attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         #     context_input_ids = batch[\"context_input_ids\"].to(device)\n",
    "#         #     context_attention_mask = batch[\"context_attention_mask\"].to(device)\n",
    "#         #     labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         for i in range(0, len(train_dataset), batch_size):\n",
    "#             # raw_batch = train_dataset[i:i + batch_size]\n",
    "\n",
    "#             # Tokenize questions and contexts within the loop\n",
    "#             # question_encodings = question_tokenizer([example[\"question\"] for example in raw_batch], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "#             # context_encodings = context_tokenizer([example[\"context\"] for example in raw_batch], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "#             # labels = torch.tensor([example[\"label\"] for example in raw_batch], dtype=torch.long).to(device)\n",
    "\n",
    "#             question_encodings = question_tokenizer([train_dataset[example_idx][\"question\"] for example_idx in range(i, i + batch_size)], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "#             context_encodings = context_tokenizer([train_dataset[example_idx][\"context\"] for example_idx in range(i, i + batch_size)], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "#             labels = torch.tensor([train_dataset[example_idx][\"labels\"] for example_idx in range(i, i + batch_size)], dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "#             input_ids = question_encodings['input_ids'].to(device)\n",
    "#             attention_mask = question_encodings['attention_mask'].to(device)\n",
    "#             context_input_ids = context_encodings['input_ids'].to(device)\n",
    "#             context_attention_mask = context_encodings['attention_mask'].to(device)\n",
    "#             question_outputs, context_outputs = model(input_ids, attention_mask, context_input_ids, context_attention_mask)\n",
    "\n",
    "#             scores = torch.matmul(question_outputs.pooler_output, context_outputs.pooler_output.T)\n",
    "\n",
    "#             one_hot_labels = F.one_hot(labels, num_classes=scores.size(1))\n",
    "#             loss = F.binary_cross_entropy_with_logits(scores, one_hot_labels.float())\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             loss = loss / gradient_accumulation_steps\n",
    "#             loss.backward()\n",
    "\n",
    "#             if (i + 1) % gradient_accumulation_steps == 0:\n",
    "#                 question_encoder_optimizer.step()\n",
    "#                 context_encoder_optimizer.step()\n",
    "#                 question_encoder_scheduler.step()\n",
    "#                 context_encoder_scheduler.step()\n",
    "\n",
    "#                 model.zero_grad()\n",
    "\n",
    "#         avg_train_loss = total_loss / number_of_batches\n",
    "#         print(f\"Training loss: {avg_train_loss}\")\n",
    "\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         number_of_batches_validation = ceil(len(validation_dataset) / batch_size)\n",
    "#         for i in range(0, len(validation_dataset), batch_size):\n",
    "\n",
    "#             # Tokenize questions and contexts within the loop\n",
    "#             # question_encodings = question_tokenizer([example[\"question\"] for example in raw_batch], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "#             # context_encodings = context_tokenizer([example[\"context\"] for example in raw_batch], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "#             # labels = torch.tensor([example[\"label\"] for example in raw_batch], dtype=torch.long).to(device)\n",
    "\n",
    "#             question_encodings = question_tokenizer([validation_dataset[example_idx][\"question\"] for example_idx in range(i, i + batch_size)], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "#             context_encodings = context_tokenizer([validation_dataset[example_idx][\"context\"] for example_idx in range(i, i + batch_size)], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "#             labels = torch.tensor([validation_dataset[example_idx][\"labels\"] for example_idx in range(i, i + batch_size)], dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "#             input_ids = question_encodings['input_ids'].to(device)\n",
    "#             attention_mask = question_encodings['attention_mask'].to(device)\n",
    "#             context_input_ids = context_encodings['input_ids'].to(device)\n",
    "#             context_attention_mask = context_encodings['attention_mask'].to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 question_outputs, context_outputs = model(input_ids, attention_mask, context_input_ids, context_attention_mask)\n",
    "#                 scores = torch.matmul(question_outputs.pooler_output, context_outputs.pooler_output.T)\n",
    "#                 one_hot_labels = F.one_hot(labels, num_classes=scores.size(1))\n",
    "#                 loss = F.binary_cross_entropy_with_logits(scores, one_hot_labels.float())\n",
    "\n",
    "#                 total_val_loss += loss.item()\n",
    "\n",
    "#         avg_val_loss = total_val_loss / number_of_batches_validation\n",
    "#         print(f\"Validation loss: {avg_val_loss}\")\n",
    "\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             best_model = DPRCombinedModel(model.question_encoder, model.context_encoder)\n",
    "#             best_model.load_state_dict(model.state_dict())\n",
    "#             torch.save(best_model.state_dict(), \"best_dpr_model.pth\")\n",
    "#             print(f\"Best model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "#     print(\"Training complete.\")\n",
    "#     return best_model\n",
    "\n",
    "# # Train the model\n",
    "# epochs = 3\n",
    "# batch_size = 4\n",
    "# learning_rates = {\n",
    "#     'question_encoder': 5e-5,\n",
    "#     'context_encoder': 5e-5\n",
    "# }\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# gradient_accumulation_steps = 4\n",
    "# num_warmup_steps = 0\n",
    "\n",
    "# combined_model = DPRCombinedModel(question_encoder, context_encoder)\n",
    "# best_model = train_dpr_model(train_dataset, validation_dataset, combined_model, question_tokenizer, context_tokenizer, epochs, batch_size, learning_rates, device, gradient_accumulation_steps, num_warmup_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_dpr_model(train_dataset, validation_dataset, model, question_tokenizer, context_tokenizer, epochs, batch_size, learning_rates, device, gradient_accumulation_steps=1, num_warmup_steps=0):\n",
    "\n",
    "    question_encoder_optimizer = AdamW(model.question_encoder.parameters(), lr=learning_rates['question_encoder'])\n",
    "    context_encoder_optimizer = AdamW(model.context_encoder.parameters(), lr=learning_rates['context_encoder'])\n",
    "    number_of_batches = ceil(len(train_dataset) / batch_size)\n",
    "    total_steps = number_of_batches * epochs // gradient_accumulation_steps\n",
    "    question_encoder_scheduler = get_linear_schedule_with_warmup(question_encoder_optimizer, num_warmup_steps, total_steps)\n",
    "    context_encoder_scheduler = get_linear_schedule_with_warmup(context_encoder_optimizer, num_warmup_steps, total_steps)\n",
    "\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    max_length = 512\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_iter = tqdm(range(0, len(train_dataset), batch_size), desc=\"Training\", ncols=100)\n",
    "\n",
    "        for i in train_iter:\n",
    "\n",
    "            question_encodings = question_tokenizer([train_dataset[example_idx][\"question\"] for example_idx in range(i, i + batch_size)], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "            context_encodings = context_tokenizer([train_dataset[example_idx][\"context\"] for example_idx in range(i, i + batch_size)], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "            labels = torch.tensor([train_dataset[example_idx][\"labels\"] for example_idx in range(i, i + batch_size)], dtype=torch.long).to(device)\n",
    "\n",
    "            input_ids = question_encodings['input_ids'].to(device)\n",
    "            attention_mask = question_encodings['attention_mask'].to(device)\n",
    "            context_input_ids = context_encodings['input_ids'].to(device)\n",
    "            context_attention_mask = context_encodings['attention_mask'].to(device)\n",
    "            question_outputs, context_outputs = model(input_ids, attention_mask, context_input_ids, context_attention_mask)\n",
    "\n",
    "            scores = torch.matmul(question_outputs.pooler_output, context_outputs.pooler_output.T)\n",
    "\n",
    "            one_hot_labels = F.one_hot(labels, num_classes=scores.size(1))\n",
    "            loss = F.binary_cross_entropy_with_logits(scores, one_hot_labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % gradient_accumulation_steps == 0:\n",
    "                question_encoder_optimizer.step()\n",
    "                context_encoder_optimizer.step()\n",
    "                question_encoder_scheduler.step()\n",
    "                context_encoder_scheduler.step()\n",
    "\n",
    "                model.zero_grad()\n",
    "            \n",
    "            train_iter.set_description(f\"Training (loss = {loss.item():.4f})\")\n",
    "            train_iter.refresh()\n",
    "\n",
    "        avg_train_loss = total_loss / number_of_batches\n",
    "        print(f\"Training loss: {avg_train_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        number_of_batches_validation = ceil(len(validation_dataset) / batch_size)\n",
    "        val_iter = tqdm(range(0, len(validation_dataset), batch_size), desc=\"Validation\", ncols=100)\n",
    "        for i in val_iter:\n",
    "\n",
    "            question_encodings = question_tokenizer([validation_dataset[example_idx][\"question\"] for example_idx in range(i, i + batch_size)], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "            context_encodings = context_tokenizer([validation_dataset[example_idx][\"context\"] for example_idx in range(i, i + batch_size)], return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "            labels = torch.tensor([validation_dataset[example_idx][\"labels\"] for example_idx in range(i, i + batch_size)], dtype=torch.long).to(device)\n",
    "\n",
    "            input_ids = question_encodings['input_ids'].to(device)\n",
    "            attention_mask = question_encodings['attention_mask'].to(device)\n",
    "            context_input_ids = context_encodings['input_ids'].to(device)\n",
    "            context_attention_mask = context_encodings['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                question_outputs, context_outputs = model(input_ids, attention_mask, context_input_ids, context_attention_mask)\n",
    "                scores = torch.matmul(question_outputs.pooler_output, context_outputs.pooler_output.T)\n",
    "                one_hot_labels = F.one_hot(labels, num_classes=scores.size(1))\n",
    "                loss = F.binary_cross_entropy_with_logits(scores, one_hot_labels.float())\n",
    "                total_val_loss += loss.item()\n",
    "            val_iter.set_description(f\"Validation (loss = {loss.item():.4f})\")\n",
    "            val_iter.refresh()\n",
    "            \n",
    "        avg_val_loss = total_val_loss / number_of_batches_validation\n",
    "        print(f\"Validation loss: {avg_val_loss}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = DPRCombinedModel(model.question_encoder, model.context_encoder)\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            torch.save(best_model.state_dict(), \"best_dpr_model.pth\")\n",
    "            print(f\"Best model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (loss = 7.4187):   0%|                               | 1/56309 [00:30<470:20:48, 30.07s/it]../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [3,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [4,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [5,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [6,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [7,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "Training (loss = 7.4187):   0%|                               | 1/56309 [00:30<480:53:22, 30.75s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb Cell 22\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m num_warmup_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m combined_model \u001b[39m=\u001b[39m DPRCombinedModel(question_encoder, context_encoder)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m best_model \u001b[39m=\u001b[39m train_dpr_model(train_dataset, validation_dataset, combined_model, question_tokenizer, context_tokenizer, epochs, batch_size, learning_rates, device, gradient_accumulation_steps, num_warmup_steps)\n",
      "\u001b[1;32m/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb Cell 22\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m gradient_accumulation_steps\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mif\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m gradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22637363652d63617665726c65652d73312e656e67722e74616d752e656475222c2275736572223a22726f68616e2e636861756468757279227d/home/grads/r/rohan.chaudhury/multidoc2dial/multidoc2dial/trial/dpr.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     question_encoder_optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 8\n",
    "learning_rates = {\n",
    "'question_encoder': 5e-5,\n",
    "'context_encoder': 5e-5\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gradient_accumulation_steps = 8\n",
    "num_warmup_steps = 0\n",
    "\n",
    "combined_model = DPRCombinedModel(question_encoder, context_encoder)\n",
    "best_model = train_dpr_model(train_dataset, validation_dataset, combined_model, question_tokenizer, context_tokenizer, epochs, batch_size, learning_rates, device, gradient_accumulation_steps, num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "# def train_dpr_model(train_dataset, gradient_accumulation_steps, per_device_train_batch_size, num_train_epochs, output_dir):\n",
    "#     combined_model = DPRCombinedModel(question_encoder, context_encoder)\n",
    "    \n",
    "#     training_args = TrainingArguments(\n",
    "#         gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         output_dir=output_dir,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"exact_match\",\n",
    "#         save_total_limit=1,\n",
    "#     )\n",
    "\n",
    "#     trainer = DPRTrainer(\n",
    "#         model=combined_model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=validation_dataset,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "# train_dpr_model(train_dataset, gradient_accumulation_steps=4, per_device_train_batch_size=4, num_train_epochs=3, output_dir=\"./dpr_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from transformers import Trainer\n",
    "\n",
    "# class DPRTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss_fct = torch.nn.CrossEntropyLoss()\n",
    "#         loss = loss_fct(logits, labels)\n",
    "        \n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples):\n",
    "#     question_encodings = question_tokenizer(examples['question'], truncation=True, padding='max_length', max_length=64)\n",
    "#     context_encodings = context_tokenizer(examples['context'], truncation=True, padding='max_length', max_length=256)\n",
    "    \n",
    "#     encodings = {\n",
    "#         'input_ids': question_encodings['input_ids'],\n",
    "#         'attention_mask': question_encodings['attention_mask'],\n",
    "#         'context_input_ids': context_encodings['input_ids'],\n",
    "#         'context_attention_mask': context_encodings['attention_mask'],\n",
    "#         'labels': examples['labels'],\n",
    "#     }\n",
    "    \n",
    "#     return encodings\n",
    "\n",
    "# train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "# validation_dataset = validation_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_dpr_model(train_dataset, gradient_accumulation_steps, per_device_train_batch_size, num_train_epochs, output_dir):\n",
    "#     training_args = TrainingArguments(\n",
    "#         gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         output_dir=output_dir,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"exact_match\",\n",
    "#         save_total_limit=1,\n",
    "#     )\n",
    "\n",
    "#     trainer = DPRTrainer(\n",
    "#         model=reader,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=validation_dataset,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "# train_dpr_model(train_dataset, gradient_accumulation_steps=4, per_device_train_batch_size=4, num_train_epochs=3, output_dir=\"./dpr_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlpproj': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6d18fc597d68f35882868caff42bd3974688c44a4bcb8df45c7c533b1bd427d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
